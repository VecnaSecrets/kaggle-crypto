{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SeparatePredictor import SeparatePredictor\n",
    "from train_tools import LSTMRegressor\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from flaml import AutoML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING fcoin_1\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.66; Val Loss: 6.09: 100%|██████████| 4500/4500 [01:36<00:00, 46.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total r2 score 0.02825482486012776\n",
      "Total mse score 6.408326148986816\n",
      "TRAINING fcoin_2\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.64; Val Loss: 2.50: 100%|██████████| 4500/4500 [01:44<00:00, 43.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total r2 score -0.030839449482138903\n",
      "Total mse score 2.2637577056884766\n",
      "TRAINING fcoin_3\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.60; Val Loss: 4.60: 100%|██████████| 4500/4500 [01:36<00:00, 46.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total r2 score -0.05088303594706489\n",
      "Total mse score 4.698777675628662\n",
      "TRAINING fcoin_4\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.27; Val Loss: 4.45: 100%|██████████| 4500/4500 [01:37<00:00, 46.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total r2 score -0.13625975815084534\n",
      "Total mse score 4.6782684326171875\n",
      "TRAINING fcoin_5\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.42; Val Loss: 2.68: 100%|██████████| 4500/4500 [01:36<00:00, 46.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total r2 score -0.06372646147231897\n",
      "Total mse score 2.7348034381866455\n",
      "TRAINING fcoin_6\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.64; Val Loss: 2.82:  12%|█▏        | 519/4500 [00:11<01:27, 45.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m predictor \u001B[38;5;241m=\u001B[39m SeparatePredictor(models \u001B[38;5;241m=\u001B[39m [LSTMRegressor], params\u001B[38;5;241m=\u001B[39mparams, data\u001B[38;5;241m=\u001B[39mdf)\n\u001B[1;32m     12\u001B[0m predictor\u001B[38;5;241m.\u001B[39mprepare()\n\u001B[0;32m---> 13\u001B[0m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m                \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m preds \u001B[38;5;241m=\u001B[39m predictor\u001B[38;5;241m.\u001B[39mpredict(df_t, device\u001B[38;5;241m=\u001B[39mDEVICE)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_t\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/Desktop/Hacks/kaggle-crypto/SeparatePredictor.py:132\u001B[0m, in \u001B[0;36mSeparatePredictor.train\u001B[0;34m(self, device, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_labels:\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTRAINING f\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_class\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m                     \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m                     \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_loader\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m                     \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m     r2, mse \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate_f(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels[\u001B[38;5;28mcls\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_ds[\u001B[38;5;28mcls\u001B[39m], device)\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal r2 score \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr2\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Hacks/kaggle-crypto/SeparatePredictor.py:171\u001B[0m, in \u001B[0;36mSeparatePredictor.train_class\u001B[0;34m(self, model, train_loader, test_loader, epochs, learning_rate, loss_fn, optimizer_cl, optimizer_params, device, writer, other_train_params)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m optimizer_cl(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptimizer_params)\n\u001B[0;32m--> 171\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_f\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43mwriter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mother_train_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Hacks/kaggle-crypto/train_tools.py:50\u001B[0m, in \u001B[0;36mTrainModel\u001B[0;34m(model, loss_fn, optimizer, train_loader, val_loader, epochs, display_on_epoch, device, writer)\u001B[0m\n\u001B[1;32m     47\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     49\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 50\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m display_on_epoch \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/Stonks/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Stonks/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "params = {\n",
    "    'input_size': 3,\n",
    "    'hidden_dim':64,\n",
    "    'n_layers': 2,\n",
    "    'device':DEVICE\n",
    "}\n",
    "df = pd.read_csv('train.csv')\n",
    "df_t = pd.read_csv('test.csv')\n",
    "\n",
    "predictor = SeparatePredictor(models = [LSTMRegressor], params=params, data=df)\n",
    "predictor.prepare()\n",
    "predictor.train(device=DEVICE, epochs=4500,\n",
    "                learning_rate=1e-4)\n",
    "preds = predictor.predict(df_t, device=DEVICE)\n",
    "print(df_t.shape)\n",
    "print(preds.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for n in predictor.models['coin_1'].parameters():\n",
    "    print(n.device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = [LSTMRegressor]\n",
    "model = model[0](**params).to(device=DEVICE)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# !git clone https://github.com/analokmaus/kuma_utils.git\n",
    "import sys\n",
    "sys.path.append(\"kuma_utils/\")\n",
    "from kuma_utils.preprocessing.imputer import LGBMImputer\n",
    "\n",
    "# imputer =  LGBMImputer(n_iter=100, verbose=False)\n",
    "# X = LGBMImputer(n_iter=100, verbose=True).fit_transform(X)\n",
    "\n",
    "class Pipeline_autoML:\n",
    "    def __init__(self, impute_strategy='mean'):\n",
    "        self.imputer = LGBMImputer(n_iter=100, verbose=True)\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame, y_raw: pd.Series = None):\n",
    "        self.imputer.fit(pd.DataFrame(X_raw))\n",
    "        self.scaler.fit(X_raw)\n",
    "\n",
    "    def transform(self, X_raw: pd.DataFrame, y_raw=None):\n",
    "        res = self.imputer.fit_transform(pd.DataFrame(X_raw)).values\n",
    "        res = self.scaler.transform(res)\n",
    "\n",
    "        if y_raw is not None:\n",
    "            return res, y_raw\n",
    "        return res\n",
    "\n",
    "    def fit_transform(self, *args, **kwargs):\n",
    "        self.fit(*args)\n",
    "        return self.transform(*args, **kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "class AutoMLpred(SeparatePredictor):\n",
    "    def prepare(self):\n",
    "        self.devided_data, self.y_devided, self.class_labels = self.separate_data(self.raw_data, self.y)\n",
    "        self.models = dict(zip(self.class_labels, self.models))\n",
    "        self.set_up_pipeline()\n",
    "\n",
    "        self.train_test_split()\n",
    "\n",
    "        self.fit_preprocess()\n",
    "        self.X_train = self.prepare_data(self.X_train)\n",
    "        self.X_test = self.prepare_data(self.X_test)\n",
    "\n",
    "    def set_up_pipeline(self):\n",
    "            \"\"\"\n",
    "            here we will define our preprocess pipeline as well as training functions\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            self.preprocess_p = dict(zip(self.class_labels, [Pipeline_autoML() for _ in range(self.num_classes)]))\n",
    "            self.train_f = None\n",
    "            self.validate_f = None\n",
    "\n",
    "    def train(self, train_params={}):\n",
    "        for cls in self.class_labels:\n",
    "            print(f\"TRAINING f{cls}\")\n",
    "            self.models[cls].fit(self.X_train[cls], self.y_train[cls], **train_params)\n",
    "\n",
    "            preds = self.models[cls].predict(self.X_train[cls])\n",
    "            r2 = r2_score(self.y_train[cls], preds)\n",
    "            mse = mean_squared_error(self.y_train[cls], preds)\n",
    "\n",
    "            print(f\"Total train r2 score {r2}\")\n",
    "            print(f\"Total train mse score {mse}\")\n",
    "\n",
    "            preds = self.models[cls].predict(self.X_test[cls])\n",
    "            r2 = r2_score(self.y_test[cls], preds)\n",
    "            mse = mean_squared_error(self.y_test[cls], preds)\n",
    "\n",
    "            print(f\"Total test r2 score {r2}\")\n",
    "            print(f\"Total test mse score {mse}\")\n",
    "\n",
    "    def predict(self, X:pd.DataFrame):\n",
    "        res = pd.DataFrame(index=X.index, data=np.zeros(X.shape[0]))\n",
    "\n",
    "        X_separated, classes = self.separate_data(X)\n",
    "        X_ds = self.prepare_data(X_separated)\n",
    "        out = {}\n",
    "        for cls in classes:\n",
    "            out[cls] = self.models[cls].predict(X_ds[cls])\n",
    "            res.loc[X[self.feat_devide] == cls] = out[cls].reshape(-1,1)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def evaluate(self):\n",
    "        preds = self.predict(self.X_test)\n",
    "        r2 = r2_score(self.df[self.target].values, preds)\n",
    "        mse = mean_squared_error(self.df[self.target].values, preds)\n",
    "\n",
    "        print(f\"Total r2 score {r2}\")\n",
    "        print(f\"Total mse score {mse}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Unnamed: 0', 'Id', 'date', 'coin_id', 'fwd_ret_3d', 'feat_1', 'feat_2',\n       'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7', 'feat_8', 'feat_9',\n       'feat_10'],\n      dtype='object')"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0069f662cc6d4f08bb33cdd9423a8b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab2a1f4b25b54a9084bbcca25ccae029"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "769e4f8a40af42c09d96e20946f1b18c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01903d5817a749ff9ec96f69dafc624c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13378b0d97394637a6967d7c706fdf9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e32f8a163c1c4eb3aa79566635cf7549"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4df162bafbf346bb9934fe02c5a51010"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11dc0bdd6b344d679835e3a0bf3c131e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b200a3220dc84156ac56f466f88ec45e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ffb9ef76d344987942f6ed27bb09003"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be8c8872c8b449fb9204ee7a66c14199"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd7c907c813545d7900afa1bb136a4a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING fcoin_1\n",
      "Total train r2 score 0.8473926210684125\n",
      "Total train mse score 0.4480944351468934\n",
      "Total test r2 score 0.09925783615161943\n",
      "Total test mse score 5.940085583456068\n",
      "TRAINING fcoin_2\n",
      "Total train r2 score 0.87103295045716\n",
      "Total train mse score 0.3804232169117927\n",
      "Total test r2 score -0.1766437494707387\n",
      "Total test mse score 2.583948746994651\n",
      "TRAINING fcoin_3\n",
      "Total train r2 score 0.9913391700682509\n",
      "Total train mse score 0.025007336321682495\n",
      "Total test r2 score -0.37133106178980135\n",
      "Total test mse score 6.131585685114709\n",
      "TRAINING fcoin_4\n"
     ]
    }
   ],
   "source": [
    "train_params = {\n",
    "    'task': \"regression\",\n",
    "    'time_budget': 20,\n",
    "    'metric': 'mse',\n",
    "    'verbose': 0,\n",
    "}\n",
    "features = ['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_8', 'feat_9', 'feat_10']\n",
    "coins = ['coin_1', 'coin_2', 'coin_3', 'coin_4']\n",
    "df_test = df.loc[df['coin_id'].isin(coins)]\n",
    "\n",
    "\n",
    "auto_ml = AutoMLpred(models=[AutoML], data=df_test, relevant_features=features)\n",
    "auto_ml.prepare()\n",
    "auto_ml.train(train_params)\n",
    "\n",
    "# automl = AutoML()\n",
    "# automl.fit(x_train, y_train, task=\"regression\", time_budget=600, metric = 'mse')\n",
    "# y_pred = automl.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.30957031, 0.87358491, 0.13943452, ..., 0.5       , 0.5       ,\n        0.56000001],\n       [0.36397879, 0.87358491, 0.08139881, ..., 0.5       , 0.5       ,\n        0.56000001],\n       [0.41838728, 0.87358491, 0.04270833, ..., 0.5       , 0.5       ,\n        0.56000001],\n       ...,\n       [0.265625  , 0.96666667, 0.6       , ..., 0.125     , 0.5625    ,\n        0.38      ],\n       [0.59375   , 0.93333333, 0.75      , ..., 0.296875  , 0.828125  ,\n        0.88      ],\n       [0.640625  , 0.93333333, 0.68333333, ..., 0.796875  , 0.609375  ,\n        0.86      ]])"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_ml.X_train['coin_5']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "              0\n0     -0.155895\n1     -0.198345\n2     -2.101173\n3      2.303399\n4      0.553546\n...         ...\n36720  0.085418\n36721 -0.111529\n36722  0.007720\n36723 -0.437889\n36724  0.029095\n\n[36725 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.155895</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.198345</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-2.101173</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.303399</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.553546</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36720</th>\n      <td>0.085418</td>\n    </tr>\n    <tr>\n      <th>36721</th>\n      <td>-0.111529</td>\n    </tr>\n    <tr>\n      <th>36722</th>\n      <td>0.007720</td>\n    </tr>\n    <tr>\n      <th>36723</th>\n      <td>-0.437889</td>\n    </tr>\n    <tr>\n      <th>36724</th>\n      <td>0.029095</td>\n    </tr>\n  </tbody>\n</table>\n<p>36725 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = auto_ml.predict(df)\n",
    "preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.23205342813902097\n",
      "3.6489872056279213\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(df['fwd_ret_3d'][32000:].values, preds[32000:])\n",
    "mse = mean_squared_error(df['fwd_ret_3d'][32000:].values, preds[32000:])\n",
    "\n",
    "print(r2)\n",
    "print(mse)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# preds = auto_ml.predict(df_t)\n",
    "preds[\"Id\"]=df_t['Id']\n",
    "preds.index = df_t['Id']\n",
    "preds[\"Predicted\"] = preds[0]\n",
    "preds.drop(0,axis=1)\n",
    "preds.to_csv('./submission.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
